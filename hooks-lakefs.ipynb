{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f9f0608-2357-4675-a49d-67b23327fbce",
   "metadata": {},
   "source": [
    "<img src=\"https://store-images.s-microsoft.com/image/apps.22094.728e1f25-a784-458f-90e1-7729049edba2.144bf785-b784-41dd-bcef-c91792108c09.f0be1bc2-af8f-49fc-ac4c-dfd9d53d9e8d\" alt=\"lakeFS logo\" width=130/> \n",
    "\n",
    "# Using [Lua Hooks](https://docs.lakefs.io/howto/hooks/lua.html) in lakeFS (similar to GitHub Actions)\n",
    "\n",
    "This notebook demonstrated how to create a pre-merge hook in lakeFS that validates the metadata before merging data into the production branch. \n",
    "\n",
    "1. Define hook configuration files and a Lua scripts for metadata validations. \n",
    "2. Perform an ETL process by creating an ingestion branch, uploading data files with metadata and atomically promoting the data to the production branch through a merge.\n",
    "3. The pre-merge hook prevents the promotion due to metadata issues, resulting in a Precondition Failed error.\n",
    "4. Attempt to change the metadata and promote it to production again. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70f5ad4b-2f64-4320-8ce8-a9104659ab9b",
   "metadata": {},
   "source": [
    "# Config\n",
    "\n",
    "**_If you're not using the provided lakeFS server and MinIO storage then change these values to match your environment_**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8377d32-d08c-4df2-9968-bc8fa8e83950",
   "metadata": {},
   "source": [
    "### lakeFS endpoint and credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "901e2512-0112-4088-86e0-8e79a30f0d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "lakefsEndPoint = 'http://lakefs:8000' # e.g. 'https://username.aws_region_name.lakefscloud.io' \n",
    "lakefsExternalEndpoint = 'http://lakefs:28220'\n",
    "lakefsAccessKey = 'V42FCGRVMK24JJ8DHUYG'\n",
    "lakefsSecretKey = 'bKhWxVF3kQoLY9kFmt91l+tDrEoZjqnWXzY9Eza'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c01623a-f6af-48d9-ab7a-e8f424a50110",
   "metadata": {},
   "source": [
    "### Object Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "df65c5d6-f95e-4904-9f17-d563452f5f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "storageNamespace = 's3://lakefs-demo-bucket' # e.g. \"s3://bucket\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e24f44d-ecec-4eb6-9e03-c141e2e763ca",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b59fc176-1ba7-4785-89c6-b012f25000a2",
   "metadata": {},
   "source": [
    "# Setup\n",
    "\n",
    "**(you shouldn't need to change anything in this section, just run it)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "804634d8-3ef4-4b7d-bd1f-f87420c48164",
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_name = \"demo\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0bf3089-0468-4ac2-aea9-0da84df1bea5",
   "metadata": {},
   "source": [
    "### Versioning Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "dedd7dfa-41bd-4496-a2d2-ec13db3cefaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "mainBranch = \"main\"\n",
    "ingestionBranch = \"dev\"\n",
    "fileName1 = \"userdata1.parquet\"\n",
    "fileName2 = \"userdata2.parquet\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e775654-6ea7-4790-8e34-b02f664e7dd3",
   "metadata": {},
   "source": [
    "### Some helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ca3a89b1-f6a3-4891-968e-f4f2962ac1ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
    "import os\n",
    "\n",
    "def print_diff(diff):\n",
    "    results = map(\n",
    "        lambda n:[n.path,n.path_type,n.size_bytes,n.type],\n",
    "        diff)\n",
    "\n",
    "    from tabulate import tabulate\n",
    "    print(tabulate(\n",
    "        results,\n",
    "        headers=['Path','Path Type','Size(Bytes)','Type']))\n",
    "\n",
    "def print_commit(log):\n",
    "    from datetime import datetime\n",
    "    from pprint import pprint\n",
    "\n",
    "    print('Message:', log.message)\n",
    "    print('ID:', log.id)\n",
    "    print('Committer:', log.committer)\n",
    "    print('Creation Date:', datetime.utcfromtimestamp(log.creation_date).strftime('%Y-%m-%d %H:%M:%S'))\n",
    "    print('Parents:', log.parents)\n",
    "    print('Metadata:')\n",
    "    pprint(log.metadata)\n",
    "\n",
    "def lakefs_ui_endpoint(lakefsEndPoint):\n",
    "    if lakefsEndPoint.startswith('http://host.docker.internal'):\n",
    "        lakefsUIEndPoint = lakefsEndPoint.replace('host.docker.internal','127.0.0.1')\n",
    "    elif lakefsEndPoint.startswith('http://lakefs'):\n",
    "        lakefsUIEndPoint = lakefsEndPoint.replace('lakefs','127.0.0.1')\n",
    "    else:\n",
    "        lakefsUIEndPoint = lakefsEndPoint\n",
    "        \n",
    "    return lakefsUIEndPoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9088dde-bbd5-458e-9033-d07744ab702a",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4a871c30-3d67-4ddc-988a-89a14b25d436",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exception reporting mode: Minimal\n"
     ]
    }
   ],
   "source": [
    "%xmode Minimal\n",
    "import os\n",
    "import lakefs\n",
    "import lakefs_sdk\n",
    "from lakefs_sdk.client import LakeFSClient\n",
    "from lakefs_sdk import models\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d583a0c2-1a77-4422-a4e6-ea068dd145b8",
   "metadata": {},
   "source": [
    "### Set environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "83b1acae-8ce5-452e-b73c-0e16535dd3ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"LAKECTL_SERVER_ENDPOINT_URL\"] = lakefsEndPoint\n",
    "os.environ[\"LAKECTL_CREDENTIALS_ACCESS_KEY_ID\"] = lakefsAccessKey\n",
    "os.environ[\"LAKECTL_CREDENTIALS_SECRET_ACCESS_KEY\"] = lakefsSecretKey"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d25f616-da6a-49d3-8c4e-af05e40e95e2",
   "metadata": {},
   "source": [
    "### Verify lakeFS credentials by getting lakeFS version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2ef90cd6-f991-49bc-8ebc-6101539e4560",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verifying lakeFS credentials‚Ä¶\n",
      "‚Ä¶‚úÖlakeFS credentials verified\n",
      "\n",
      "‚ÑπÔ∏èlakeFS version 1.43.0\n"
     ]
    }
   ],
   "source": [
    "print(\"Verifying lakeFS credentials‚Ä¶\")\n",
    "try:\n",
    "    v=lakefs.client.Client().version\n",
    "except:\n",
    "    print(\"üõë failed to get lakeFS version\")\n",
    "else:\n",
    "    print(f\"‚Ä¶‚úÖlakeFS credentials verified\\n\\n‚ÑπÔ∏èlakeFS version {v}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a787833c-8f4d-47ac-91a7-07921e22dd22",
   "metadata": {},
   "source": [
    "Working with the lakeFS Python client API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1ab7ad27-0d95-469d-bf33-819c833121c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "configuration = lakefs_sdk.Configuration(\n",
    "    host=lakefsEndPoint,\n",
    "    username=lakefsAccessKey,\n",
    "    password=lakefsSecretKey,\n",
    ")\n",
    "lakefsClient = LakeFSClient(configuration)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f8400f8-575b-461d-9117-91cf948b8643",
   "metadata": {},
   "source": [
    "### Define lakeFS Repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "abf9016f-eeab-424e-bf83-6971feaf6dfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 'demo', 'creation_date': 1732532388, 'default_branch': 'main', 'storage_namespace': 's3://lakefs-demo-bucket/demo'}\n"
     ]
    }
   ],
   "source": [
    "repo = lakefs.Repository(repo_name).create(storage_namespace=f\"{storageNamespace}/{repo_name}\", default_branch=mainBranch, exist_ok=True)\n",
    "branchMain = repo.branch(mainBranch)\n",
    "print(repo)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e717be-fe80-401f-955e-13a0467e05ed",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85ad17db-6c40-421e-8022-0015bcbbb3bc",
   "metadata": {},
   "source": [
    "# Main demo starts here üö¶ üëáüèª"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef97145-9fb0-4950-bf6a-50a44b1bb542",
   "metadata": {},
   "source": [
    "## Setup and Configure Hooks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ff332c-31e1-4b95-9ca5-826792f6b9f5",
   "metadata": {},
   "source": [
    "### Configure hooks in the repository\n",
    "\n",
    "* Upload [Hooks config YAML file](./hooks/pre-merge-metadata-validation.yaml) for metadata validation to check for mandatory metadata before data is merged into the main branch\n",
    "* Hooks config file must be uploaded to \"_lakefs_actions\" prefix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "009dd253-f2aa-4b7f-bf9f-253937c1105f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_lakefs_actions/pre-merge-metadata-validation.yaml\n"
     ]
    }
   ],
   "source": [
    "hooks_config_yaml = \"pre-merge-metadata-validation.yaml\"\n",
    "hooks_prefix = \"_lakefs_actions\"\n",
    "\n",
    "contentToUpload = open(f'./hooks/{hooks_config_yaml}', 'r').read()\n",
    "print(branchMain.object(f'{hooks_prefix}/{hooks_config_yaml}').upload(data=contentToUpload, mode='wb', pre_sign=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85099b4a-7700-445c-88dd-cd2eb0dab3c9",
   "metadata": {},
   "source": [
    "### Upload 1st script\n",
    "\n",
    "##### The script [commit_metadata_validator.lua](./hooks/commit_metadata_validator.lua) checks commit metadata to validate that mandatory metadata fields are present and value for the metadata fields match the required pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b20b7ce3-dc8c-438a-a6a1-717430c2d191",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scripts/commit_metadata_validator.lua\n"
     ]
    }
   ],
   "source": [
    "lua_script_file_name = \"commit_metadata_validator.lua\"\n",
    "lua_scripts_path = \"scripts\"\n",
    "\n",
    "contentToUpload = open(f'./hooks/{lua_script_file_name}', 'r').read()\n",
    "print(branchMain.object(f'{lua_scripts_path}/{lua_script_file_name}').upload(data=contentToUpload, mode='wb', pre_sign=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8574f329-1e87-476d-9ccb-15636cb3ae3d",
   "metadata": {},
   "source": [
    "### Upload 2nd script\n",
    "\n",
    "##### The script [dataset_validator.lua](./hooks/dataset_validator.lua) validates the existence of mandatory metadata describing a dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a52ed281-a28f-40f3-8e3c-b23f0a845015",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scripts/dataset_validator.lua\n"
     ]
    }
   ],
   "source": [
    "lua_script_file_name = \"dataset_validator.lua\"\n",
    "lua_scripts_path = \"scripts\"\n",
    "\n",
    "contentToUpload = open(f'./hooks/{lua_script_file_name}', 'r').read()\n",
    "print(branchMain.object(f'{lua_scripts_path}/{lua_script_file_name}').upload(data=contentToUpload, mode='wb', pre_sign=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9693fe1e-6550-4e01-a9b6-79ebc9c6f33c",
   "metadata": {},
   "source": [
    "### Commit changes to the lakeFS repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1c917918-724c-45c0-aa2f-4aa240706eb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message: Added hooks config file and metadata validation scripts\n",
      "ID: a672fc9595fb01dd46586f82788bf300b1ff3af668c254a1e4f89f5a6c233151\n",
      "Committer: quickstart\n",
      "Creation Date: 2024-11-25 11:00:34\n",
      "Parents: ['fbab0e2b947ddcaec1d5acf4bf74aa26c8449e2d3edecf45939119331536aef7']\n",
      "Metadata:\n",
      "{}\n"
     ]
    }
   ],
   "source": [
    "ref = branchMain.commit(message='Added hooks config file and metadata validation scripts')\n",
    "print_commit(ref.get_commit())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6a811e7-f2d2-4e26-a7bd-6aa84934b5eb",
   "metadata": {},
   "source": [
    "### Protect main branch so no one can write directly to the main branch and any subsequent writes must be done via the merge of a branch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2ad30273-6997-4909-be88-ebfa0a8fc28a",
   "metadata": {},
   "outputs": [],
   "source": [
    "lakefsClient.repositories_api.set_branch_protection_rules(\n",
    "    repository=repo_name,\n",
    "    branch_protection_rule=[models.BranchProtectionRule(\n",
    "        pattern=mainBranch)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1af47a57-706b-4914-b7a9-3e00c2e78958",
   "metadata": {},
   "source": [
    "# ETL Job Starts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "618e7eff-64ed-4f22-bf51-0157ab3521fd",
   "metadata": {},
   "source": [
    "## Create a new branch which will be used to ingest data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "86f0d64e-d65f-4275-9765-232207d9a448",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dev ref: a672fc9595fb01dd46586f82788bf300b1ff3af668c254a1e4f89f5a6c233151\n"
     ]
    }
   ],
   "source": [
    "branchIngestion = repo.branch(ingestionBranch).create(source_reference=mainBranch, exist_ok=True)\n",
    "print(f\"{ingestionBranch} ref:\", branchIngestion.get_commit().id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94070c58-c7fd-4021-8c2a-7aca3ef8270d",
   "metadata": {},
   "source": [
    "## Upload data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "0abff4db-23f1-4b86-81b0-2c6efda46302",
   "metadata": {},
   "outputs": [],
   "source": [
    "obj = branchIngestion.object(path=f\"datasets/{fileName1}\")\n",
    "\n",
    "with open(f\"./data/{fileName1}\", mode='rb') as reader, obj.writer(mode='wb') as writer:\n",
    "    writer.write(reader.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f219ed8d-a4e3-4573-b1d4-81783199a04e",
   "metadata": {},
   "outputs": [],
   "source": [
    "obj = branchIngestion.object(path=f\"datasets/{fileName2}\")\n",
    "\n",
    "with open(f\"./data/{fileName2}\", mode='rb') as reader, obj.writer(mode='wb') as writer:\n",
    "    writer.write(reader.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e16bf8b1-e5fc-417d-a6eb-057440826817",
   "metadata": {},
   "source": [
    "## Upload metadata file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "9e92d5ed-a1b2-47f1-997f-6f201be58a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_metadata_definition = {\n",
    "   'contains_pii': 'yes',\n",
    "   'rank': 1,\n",
    "   'department': 'finance'\n",
    "}\n",
    "\n",
    "with branchIngestion.object(path='datasets/dataset_metadata.yaml').writer() as out:\n",
    "   yaml.safe_dump(dataset_metadata_definition, out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daef0500-8d84-4d25-b6aa-39806227c3b2",
   "metadata": {},
   "source": [
    "## Commit changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "9f703a63-1be5-4ff5-9abe-b534e1ce9a38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message: Added data and metadata files\n",
      "ID: 71dbb339f232ce7252113e18146ba8bda3d91985ac3d42ef6bd692a89bbb04ae\n",
      "Committer: quickstart\n",
      "Creation Date: 2024-11-25 11:04:44\n",
      "Parents: ['a672fc9595fb01dd46586f82788bf300b1ff3af668c254a1e4f89f5a6c233151']\n",
      "Metadata:\n",
      "{}\n"
     ]
    }
   ],
   "source": [
    "ref = branchIngestion.commit(message='Added data and metadata files')\n",
    "print_commit(ref.get_commit())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eef4304-bf35-4e67-9d7a-cef444de7800",
   "metadata": {},
   "source": [
    "## Promote the Data into production\n",
    "\n",
    "#### Merging the ingestion branch with the current metadata to the production branch\n",
    "#### üõëüõë Merge will fail because 'spark_version' metadata key is missing in the merge metadata.  Review the error message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "71497275-9e2c-45c6-b5ae-581ca24c3978",
   "metadata": {},
   "outputs": [
    {
     "ename": "ServerException",
     "evalue": "code: 412, reason: Precondition Failed, body: {'message': '1 error occurred:\\n\\t* hook run id \\'0000_0000\\' failed on action \\'Validate Commit Metadata and Dataset Metadata Fields\\' hook \\'check_commit_metadata\\': runtime error: [string \"lua\"]:33: missing mandatory metadata field: spark_version\\nstack traceback:\\n\\t[Go]: in function \\'__index\\'\\n\\t[string \"lua\"]:33: in main chunk\\n\\n'}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31mServerException\u001b[0m\u001b[0;31m:\u001b[0m code: 412, reason: Precondition Failed, body: {'message': '1 error occurred:\\n\\t* hook run id \\'0000_0000\\' failed on action \\'Validate Commit Metadata and Dataset Metadata Fields\\' hook \\'check_commit_metadata\\': runtime error: [string \"lua\"]:33: missing mandatory metadata field: spark_version\\nstack traceback:\\n\\t[Go]: in function \\'__index\\'\\n\\t[string \"lua\"]:33: in main chunk\\n\\n'}\n"
     ]
    }
   ],
   "source": [
    "res = branchIngestion.merge_into(branchMain, \n",
    "        metadata={'notebook_url': 'https://github.com/treeverse/lakeFS-samples/blob/main/00_notebooks/hooks-metadata-validation.ipynb'})\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "583a58fb-1b07-48b0-80d8-d7bb5b4efc78",
   "metadata": {},
   "source": [
    "#### Add 'spark_version' metadata and try to merge again.\n",
    "#### üõëüõë Merge will fail again because metadata field 'notebook_url' does not match the pattern: 'github.com/.*'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "17c56ddb-b667-4c91-8ae1-688cff1cc909",
   "metadata": {},
   "outputs": [
    {
     "ename": "ServerException",
     "evalue": "code: 412, reason: Precondition Failed, body: {'message': '1 error occurred:\\n\\t* hook run id \\'0000_0000\\' failed on action \\'Validate Commit Metadata and Dataset Metadata Fields\\' hook \\'check_commit_metadata\\': runtime error: [string \"lua\"]:36: current value for commit metadata field notebook_url does not match pattern: github.com/.* - got: https://github.ai/treeverse/lakeFS-samples/blob/main/00_notebooks/hooks-metadata-validation.ipynb\\nstack traceback:\\n\\t[Go]: in function \\'for iterator\\'\\n\\t[string \"lua\"]:36: in main chunk\\n\\n'}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31mServerException\u001b[0m\u001b[0;31m:\u001b[0m code: 412, reason: Precondition Failed, body: {'message': '1 error occurred:\\n\\t* hook run id \\'0000_0000\\' failed on action \\'Validate Commit Metadata and Dataset Metadata Fields\\' hook \\'check_commit_metadata\\': runtime error: [string \"lua\"]:36: current value for commit metadata field notebook_url does not match pattern: github.com/.* - got: https://github.ai/treeverse/lakeFS-samples/blob/main/00_notebooks/hooks-metadata-validation.ipynb\\nstack traceback:\\n\\t[Go]: in function \\'for iterator\\'\\n\\t[string \"lua\"]:36: in main chunk\\n\\n'}\n"
     ]
    }
   ],
   "source": [
    "res = branchIngestion.merge_into(branchMain, \n",
    "        metadata={'notebook_url': 'https://github.ai/treeverse/lakeFS-samples/blob/main/00_notebooks/hooks-metadata-validation.ipynb',\n",
    "                 'spark_version': '3.3.2'})\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "352ecd91-7eea-4c38-9949-ec975d7539fb",
   "metadata": {},
   "source": [
    "#### Change 'github.ai' to 'github.com' in the value of 'notebook_url' metadata and try to merge again.\n",
    "#### üõëüõë Merge will fail again because field 'contains_pii' in dataset_metadata.yaml file should be of type boolean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a6fc5612-5766-446f-8353-21d7dcc09137",
   "metadata": {},
   "outputs": [
    {
     "ename": "ServerException",
     "evalue": "code: 412, reason: Precondition Failed, body: {'message': \"1 error occurred:\\n\\t* validate_datasets: datasets/dataset_metadata.yaml: field 'contains_pii' should be of type boolean\\n\\n\"}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31mServerException\u001b[0m\u001b[0;31m:\u001b[0m code: 412, reason: Precondition Failed, body: {'message': \"1 error occurred:\\n\\t* validate_datasets: datasets/dataset_metadata.yaml: field 'contains_pii' should be of type boolean\\n\\n\"}\n"
     ]
    }
   ],
   "source": [
    "res = branchIngestion.merge_into(branchMain, \n",
    "        metadata={'notebook_url': 'https://github.com/treeverse/lakeFS-samples/blob/main/00_notebooks/hooks-metadata-validation.ipynb',\n",
    "                 'spark_version': '3.3.2'})\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08b5c462-7a30-4aa4-9b39-a5e25294200b",
   "metadata": {},
   "source": [
    "#### Change value for the field 'contains_pii' in dataset_metadata.yaml file to 'True' and try to merge again.\n",
    "#### üõëüõë Merge will fail again because field 'approval_link' is required in the dataset_metadata.yaml file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "82cc4a30-d4de-4237-a5c2-0f3f43040675",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message: Changed metadata file\n",
      "ID: 18ee652360b8a44c666dd5e33563b6bcb9270bf0b2ecac89c6df0acc1fdf7c20\n",
      "Committer: quickstart\n",
      "Creation Date: 2024-11-25 11:07:29\n",
      "Parents: ['71dbb339f232ce7252113e18146ba8bda3d91985ac3d42ef6bd692a89bbb04ae']\n",
      "Metadata:\n",
      "{}\n"
     ]
    }
   ],
   "source": [
    "dataset_metadata_definition = {\n",
    "   'contains_pii': True,\n",
    "   'rank': 1,\n",
    "   'department': 'finance'\n",
    "}\n",
    "\n",
    "with branchIngestion.object(path='datasets/dataset_metadata.yaml').writer() as out:\n",
    "   yaml.safe_dump(dataset_metadata_definition, out)\n",
    "\n",
    "ref = branchIngestion.commit(message='Changed metadata file')\n",
    "print_commit(ref.get_commit())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "87592dc6-5f3c-4d0c-bd2b-7d48c317fdd5",
   "metadata": {},
   "outputs": [
    {
     "ename": "ServerException",
     "evalue": "code: 412, reason: Precondition Failed, body: {'message': \"1 error occurred:\\n\\t* validate_datasets: datasets/dataset_metadata.yaml: field 'approval_link' is required but no value given\\n\\n\"}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31mServerException\u001b[0m\u001b[0;31m:\u001b[0m code: 412, reason: Precondition Failed, body: {'message': \"1 error occurred:\\n\\t* validate_datasets: datasets/dataset_metadata.yaml: field 'approval_link' is required but no value given\\n\\n\"}\n"
     ]
    }
   ],
   "source": [
    "res = branchIngestion.merge_into(branchMain, \n",
    "        metadata={'notebook_url': 'https://github.com/treeverse/lakeFS-samples/blob/main/00_notebooks/hooks-metadata-validation.ipynb',\n",
    "                 'spark_version': '3.3.2'})\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e125580e-6120-40be-8bf1-0d642a7564e1",
   "metadata": {},
   "source": [
    "#### Add field 'approval_link' in the dataset_metadata.yaml file and try to merge again.\n",
    "#### üõëüõë Merge will fail again because value for field 'approval_link' should match the pattern 'https?:\\\\/\\\\/.*'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "50df66e1-61d8-4acf-8046-da1f5b694f25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message: Changed metadata file\n",
      "ID: 8569cab26cfb27784303e009f31916f9e5a737b9ca501d4ae946ed1118f02baf\n",
      "Committer: quickstart\n",
      "Creation Date: 2024-11-25 11:08:48\n",
      "Parents: ['18ee652360b8a44c666dd5e33563b6bcb9270bf0b2ecac89c6df0acc1fdf7c20']\n",
      "Metadata:\n",
      "{}\n"
     ]
    }
   ],
   "source": [
    "dataset_metadata_definition = {\n",
    "   'contains_pii': True,\n",
    "   'approval_link': 'example.com',\n",
    "   'rank': 1,\n",
    "   'department': 'finance'\n",
    "}\n",
    "\n",
    "with branchIngestion.object(path='datasets/dataset_metadata.yaml').writer() as out:\n",
    "   yaml.safe_dump(dataset_metadata_definition, out)\n",
    "\n",
    "ref = branchIngestion.commit(message='Changed metadata file')\n",
    "print_commit(ref.get_commit())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "1562b7f6-6d5f-4c93-bcc3-b02268b631d4",
   "metadata": {},
   "outputs": [
    {
     "ename": "ServerException",
     "evalue": "code: 412, reason: Precondition Failed, body: {'message': \"1 error occurred:\\n\\t* validate_datasets: datasets/dataset_metadata.yaml: field approval_link should match pattern 'https?:\\\\/\\\\/.*'\\n\\n\"}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31mServerException\u001b[0m\u001b[0;31m:\u001b[0m code: 412, reason: Precondition Failed, body: {'message': \"1 error occurred:\\n\\t* validate_datasets: datasets/dataset_metadata.yaml: field approval_link should match pattern 'https?:\\\\/\\\\/.*'\\n\\n\"}\n"
     ]
    }
   ],
   "source": [
    "res = branchIngestion.merge_into(branchMain, \n",
    "        metadata={'notebook_url': 'https://github.com/treeverse/lakeFS-samples/blob/main/00_notebooks/hooks-metadata-validation.ipynb',\n",
    "                 'spark_version': '3.3.2'})\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db84118-5e71-424b-918e-5257b0bd4ede",
   "metadata": {},
   "source": [
    "#### Change value for the field 'approval_link' from 'example.com' to 'https://example.com' and try to merge again.\n",
    "#### üõëüõë Merge will fail again because value for the field 'department' should be one of 'hr, it, other'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "546d1014-bda4-4985-9900-53d4709ea14a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message: Changed metadata file\n",
      "ID: 05f0a2769abc8dbe292f4966a58eeac39fc0c022297cacdd2f86f7a632a686e9\n",
      "Committer: quickstart\n",
      "Creation Date: 2024-11-25 11:09:11\n",
      "Parents: ['8569cab26cfb27784303e009f31916f9e5a737b9ca501d4ae946ed1118f02baf']\n",
      "Metadata:\n",
      "{}\n"
     ]
    }
   ],
   "source": [
    "dataset_metadata_definition = {\n",
    "   'contains_pii': True,\n",
    "   'approval_link': 'https://example.com',\n",
    "   'rank': 1,\n",
    "   'department': 'finance'\n",
    "}\n",
    "\n",
    "with branchIngestion.object(path='datasets/dataset_metadata.yaml').writer() as out:\n",
    "   yaml.safe_dump(dataset_metadata_definition, out)\n",
    "\n",
    "ref = branchIngestion.commit(message='Changed metadata file')\n",
    "print_commit(ref.get_commit())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "8191c71f-66f9-453f-9efb-88febb07bf24",
   "metadata": {},
   "outputs": [
    {
     "ename": "ServerException",
     "evalue": "code: 412, reason: Precondition Failed, body: {'message': \"1 error occurred:\\n\\t* validate_datasets: datasets/dataset_metadata.yaml: field 'department' should be one of 'hr, it, other'\\n\\n\"}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31mServerException\u001b[0m\u001b[0;31m:\u001b[0m code: 412, reason: Precondition Failed, body: {'message': \"1 error occurred:\\n\\t* validate_datasets: datasets/dataset_metadata.yaml: field 'department' should be one of 'hr, it, other'\\n\\n\"}\n"
     ]
    }
   ],
   "source": [
    "res = branchIngestion.merge_into(branchMain, \n",
    "        metadata={'notebook_url': 'https://github.com/treeverse/lakeFS-samples/blob/main/00_notebooks/hooks-metadata-validation.ipynb',\n",
    "                 'spark_version': '3.3.2'})\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "663f7466-9c93-4ad2-a242-fa90b7bad4cd",
   "metadata": {},
   "source": [
    "#### Change value for the field 'department' from 'finance' to 'hr' and try to merge again.\n",
    "#### Merge will succeed this time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "83ec6433-3d92-44cf-8834-45999487587a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message: Changed metadata file\n",
      "ID: f557d8996434387502c5c3c83c72b33b9e08f36165757bdc84c446c9548fe7ca\n",
      "Committer: quickstart\n",
      "Creation Date: 2024-11-25 11:09:42\n",
      "Parents: ['05f0a2769abc8dbe292f4966a58eeac39fc0c022297cacdd2f86f7a632a686e9']\n",
      "Metadata:\n",
      "{}\n"
     ]
    }
   ],
   "source": [
    "dataset_metadata_definition = {\n",
    "   'contains_pii': True,\n",
    "   'approval_link': 'https://example.com',\n",
    "   'rank': 1,\n",
    "   'department': 'hr'\n",
    "}\n",
    "\n",
    "with branchIngestion.object(path='datasets/dataset_metadata.yaml').writer() as out:\n",
    "   yaml.safe_dump(dataset_metadata_definition, out)\n",
    "\n",
    "ref = branchIngestion.commit(message='Changed metadata file')\n",
    "print_commit(ref.get_commit())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "aaf82c46-4d26-46de-8bbe-9e6485968079",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fa90c522501cb87e916880bdddbc8ae37670cfdcf6611b24cdce8dd899daec8a\n"
     ]
    }
   ],
   "source": [
    "res = branchIngestion.merge_into(branchMain, \n",
    "        metadata={'notebook_url': 'https://github.com/treeverse/lakeFS-samples/blob/main/00_notebooks/hooks-metadata-validation.ipynb',\n",
    "                 'spark_version': '3.3.2'})\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9e6bf5e-d538-49be-a384-9943d53e7754",
   "metadata": {},
   "source": [
    "## You can also review all Actions in lakeFS UI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "03cb4f79-b79d-4ba3-8806-9ff3838d054e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üëâüèª http://127.0.0.1:28220/repositories/demo/actions\n"
     ]
    }
   ],
   "source": [
    "lakefsUIEndPoint = lakefs_ui_endpoint(lakefsEndPoint)\n",
    "print(f\"üëâüèª {lakefsUIEndPoint}/repositories/{repo_name}/actions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0cbcaa6-9c7d-4ec9-bd24-75254306012f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
